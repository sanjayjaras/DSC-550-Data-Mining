{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "denclue.py\n",
    "\n",
    "@author: mgarrett\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClusterMixin\n",
    "import networkx as nx\n",
    "import concurrent.futures as cf\n",
    "\n",
    "def _hill_climb(x_t, X, h=0.1, eps=1e-7):\n",
    "    \"\"\"\n",
    "    This function climbs the 'hill' of the kernel density function\n",
    "    and finds the 'peak', which represents the density attractor\n",
    "    \"\"\"\n",
    "    error = 99.\n",
    "    prob = 0.\n",
    "    x_l1 = np.copy(x_t)\n",
    "    \n",
    "    #Sum of the last three steps is used to establish radius\n",
    "    #of neighborhood around attractor. Authors suggested two\n",
    "    #steps works well, but I found three is more robust to\n",
    "    #noisey datasets.\n",
    "    radius_new = 0.\n",
    "    radius_old = 0.\n",
    "    radius_twiceold = 0.\n",
    "    iters = 0.\n",
    "    while True:\n",
    "        radius_thriceold = radius_twiceold\n",
    "        radius_twiceold = radius_old\n",
    "        radius_old = radius_new\n",
    "        x_l0 = np.copy(x_l1)\n",
    "        x_l1, density = _step(x_l0, X, h=h)\n",
    "        error = density - prob\n",
    "        prob = density\n",
    "        radius_new = np.linalg.norm(x_l1-x_l0)\n",
    "        radius = radius_thriceold + radius_twiceold + radius_old + radius_new\n",
    "        iters += 1\n",
    "        if iters>3 and radius_new > eps:\n",
    "            break\n",
    "    return [x_l1, prob, radius]\n",
    "\n",
    "def _step(x_l0, X, h=0.1):\n",
    "    n = X.shape[0]\n",
    "    d = X.shape[1]\n",
    "    superweight = 0. #superweight is the kernel X weight for each item\n",
    "    x_l1 = np.zeros((1,d))\n",
    "    for i in range(n):\n",
    "        kernel = kernelize(x_l0, X[i], h, d)\n",
    "        kernel = kernel * 1/(h**d)\n",
    "        superweight = superweight + kernel\n",
    "        x_l1 = x_l1 + (kernel * X[i])\n",
    "    x_l1 = x_l1/superweight\n",
    "    density = superweight/n\n",
    "    return [x_l1, density]\n",
    "    \n",
    "def kernelize(x, y, h, degree):\n",
    "    kernel = np.exp(-(np.linalg.norm(x-y)/h)**2./2.)/((2.*np.pi)**(degree/2))\n",
    "    return kernel\n",
    "\n",
    "class DENCLUE(BaseEstimator, ClusterMixin):\n",
    "    \"\"\"Perform DENCLUE clustering from vector array.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    h : float, optional\n",
    "        The smoothing parameter for the gaussian kernel. This is a hyper-\n",
    "        parameter, and the optimal value depends on data. Default is the\n",
    "        np.std(X)/5.\n",
    "\n",
    "    eps : float, optional\n",
    "        Convergence threshold parameter for density attractors\n",
    "\n",
    "    min_density : float, optional\n",
    "        The minimum kernel density required for a cluster attractor to be\n",
    "        considered a cluster and not noise.  Cluster info will stil be kept\n",
    "        but the label for the corresponding instances will be -1 for noise.\n",
    "        Since what consitutes a high enough kernel density depends on the\n",
    "        nature of the data, it's often best to fit the model first and \n",
    "        explore the results before deciding on the min_density, which can be\n",
    "        set later with the 'set_minimum_density' method.\n",
    "        Default is 0.\n",
    "\n",
    "    metric : string, or callable\n",
    "        The metric to use when calculating distance between instances in a\n",
    "        feature array. In this version, I've only tested 'euclidean' at this\n",
    "        moment.\n",
    "\n",
    "    Attributes\n",
    "    -------\n",
    "    cluster_info_ : dictionary [n_clusters]\n",
    "        Contains relevant information of all clusters (i.e. density attractors)\n",
    "        Information is retained even if the attractor is lower than the\n",
    "        minimum density required to be labelled a cluster.\n",
    "\n",
    "    labels_ : array [n_samples]\n",
    "        Cluster labels for each point.  Noisy samples are given the label -1.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    Hinneburg A., Gabriel HH. \"DENCLUE 2.0: Fast Clustering Based on Kernel \n",
    "    Density Estimation\". In: R. Berthold M., Shawe-Taylor J., LavraÄ N. (eds)\n",
    "    Advances in Intelligent Data Analysis VII. IDA 2007\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, h=None, eps=1e-8, min_density=0., metric='euclidean'):        \n",
    "        self.h = h        \n",
    "        self.eps = eps\n",
    "        self.min_density = min_density\n",
    "        self.metric = metric\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        if not self.eps > 0.0:\n",
    "            raise ValueError(\"eps must be positive.\")\n",
    "        self.n_samples = X.shape[0]\n",
    "        self.n_features = X.shape[1]\n",
    "        density_attractors = np.zeros((self.n_samples,self.n_features))\n",
    "        radii = np.zeros((self.n_samples,1))\n",
    "        density = np.zeros((self.n_samples,1))\n",
    "        \n",
    "        #create default values\n",
    "        if self.h is None:\n",
    "            self.h = np.std(X)/5\n",
    "\n",
    "        \n",
    "        #initialize all labels to noise\n",
    "        labels = -np.ones(X.shape[0])\n",
    "\n",
    "        dummy_x = [X for _ in range(len(X))]\n",
    "        dummy_h = [self.h for _ in range(len(X))]\n",
    "        dummy_eps= [self.eps for _ in range(len(X))]\n",
    "\n",
    "        with cf.ProcessPoolExecutor() as executor:\n",
    "            results = executor.map(_hill_climb, X, dummy_x, dummy_h, dummy_eps)\n",
    "\n",
    "        i = 0\n",
    "        for result in results:\n",
    "            density_attractors[i] = result[0]\n",
    "            density[i] = result[1]\n",
    "            radii[i] = result[2]\n",
    "            i += 1\n",
    "\n",
    "        print(\"Done with finding density attractors....\")\n",
    "        #climb each hill\n",
    "        #for i in range(self.n_samples):\n",
    "        #    density_attractors[i], density[i], radii[i] = _hill_climb(X[i], X, h=self.h, eps=self.eps)\n",
    "            \n",
    "        #initialize cluster graph to finalize clusters. Networkx graph is\n",
    "        #used to verify clusters, which are connected components of the\n",
    "        #graph. Edges are defined as density attractors being in the same\n",
    "        #neighborhood as defined by our radii for each attractor.\n",
    "        cluster_info = {}\n",
    "        num_clusters = 0\n",
    "        cluster_info[num_clusters]={'instances': [0],\n",
    "                                    'centroid': np.atleast_2d(density_attractors[0])}\n",
    "        g_clusters = nx.Graph()\n",
    "        for j1 in range(self.n_samples):\n",
    "            g_clusters.add_node(j1, attr_dict={'attractor':density_attractors[j1], 'radius':radii[j1],\n",
    "                                'density':density[j1]})\n",
    "        print(\"Completed creating graph...\")\n",
    "        #populate cluster graph\n",
    "        for j1 in range(self.n_samples):\n",
    "            for j2 in (x for x in range(self.n_samples) if x != j1):\n",
    "                if g_clusters.has_edge(j1,j2):\n",
    "                    continue\n",
    "                #print(g_clusters.node[j1])    \n",
    "                diff = np.linalg.norm(g_clusters.node[j1]['attr_dict']['attractor']-g_clusters.node[j2]['attr_dict']['attractor'])\n",
    "                if diff <= (g_clusters.node[j1]['attr_dict']['radius']+g_clusters.node[j1]['attr_dict']['radius']):\n",
    "                    g_clusters.add_edge(j1, j2)\n",
    "                    \n",
    "        #connected components represent a cluster\n",
    "        clusters = list(nx.connected_component_subgraphs(g_clusters))\n",
    "        num_clusters = 0\n",
    "        \n",
    "        #loop through all connected components\n",
    "        for clust in clusters:\n",
    "            \n",
    "            #get maximum density of attractors and location\n",
    "            max_instance = max(clust, key=lambda x: clust.node[x]['attr_dict']['density'])\n",
    "            max_density = clust.node[max_instance]['attr_dict']['density']\n",
    "            max_centroid = clust.node[max_instance]['attr_dict']['attractor']\n",
    "            \n",
    "           \n",
    "            #In Hinneberg, Gabriel (2007), for attractors in a component that\n",
    "            #are not fully connected (i.e. not all attractors are within each\n",
    "            #other's neighborhood), they recommend re-running the hill climb \n",
    "            #with lower eps. From testing, this seems unnecesarry for all but\n",
    "            #special edge cases. Therefore, completeness info is put into \n",
    "            #cluster info dict, but not used to re-run hill climb.\n",
    "            complete = False\n",
    "            c_size = len(clust.nodes())\n",
    "            if clust.number_of_edges() == (c_size*(c_size-1))/2.:\n",
    "                complete = True\n",
    "            \n",
    "            #populate cluster_info dict\n",
    "            cluster_info[num_clusters] = {'instances': clust.nodes(),\n",
    "                                        'size': c_size,\n",
    "                                        'centroid': max_centroid,\n",
    "                                        'density': max_density,\n",
    "                                        'complete': complete}\n",
    "            \n",
    "            #if the cluster density is not higher than the minimum,\n",
    "            #instances are kept classified as noise\n",
    "            if max_density >= self.min_density:\n",
    "                labels[clust.nodes()]=num_clusters            \n",
    "            num_clusters += 1\n",
    "\n",
    "        self.clust_info_ = cluster_info\n",
    "        self.labels_ = labels\n",
    "        return self\n",
    "        \n",
    "    def get_density(self, x, X, y=None, sample_weight=None):\n",
    "        superweight=0.\n",
    "        n_samples = X.shape[0]\n",
    "        n_features = X.shape[1]\n",
    "        if sample_weight is None:\n",
    "            sample_weight = np.ones((n_samples,1))\n",
    "        else:\n",
    "            sample_weight = sample_weight\n",
    "        for y in range(n_samples):\n",
    "            kernel = kernelize(x, X[y], h=self.h, degree=n_features)\n",
    "            kernel = kernel * sample_weight[y]/(self.h**n_features)\n",
    "            superweight = superweight + kernel\n",
    "        density = superweight/np.sum(sample_weight)\n",
    "        return density\n",
    "        \n",
    "    def set_minimum_density(self, min_density):\n",
    "        self.min_density = min_density\n",
    "        labels_copy = np.copy(self.labels_)\n",
    "        for k in self.clust_info_.keys():\n",
    "            if self.clust_info_[k]['density']<min_density:\n",
    "                labels_copy[self.clust_info_[k]['instances']]= -1\n",
    "            else:\n",
    "                labels_copy[self.clust_info_[k]['instances']]= k\n",
    "        self.labels_ = labels_copy\n",
    "        return self\n",
    "\n",
    "dataset = pd.read_csv(\"iris.txt\", header=None, names=[\"x1\", \"x2\", \"x3\", \"x4\", \"x5\"])\n",
    "\n",
    "denclue = DENCLUE(min_density=0, eps=0.0001, h=0.28)\n",
    "denclue.fit(np.array(dataset.iloc[:, [0, 1, 2, 3]]))\n",
    "print(denclue)\n",
    "print(len(denclue.clust_info_))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-746e0e06",
   "language": "python",
   "display_name": "PyCharm (code)"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}